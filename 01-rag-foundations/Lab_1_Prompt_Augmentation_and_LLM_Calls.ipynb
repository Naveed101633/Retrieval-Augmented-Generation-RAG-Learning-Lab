{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDL49XhEBMY3kJsP2TbUBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naveed101633/Retrieval-Augmented-Generation-RAG-Learning-Lab/blob/main/01-rag-foundations/Lab_1_Prompt_Augmentation_and_LLM_Calls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GLR3FXpHPKVT"
      },
      "outputs": [],
      "source": [
        "# Install the Google GenAI SDK\n",
        "!pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Setup Client\n",
        "API_KEY = 'AIzaSyAkIrI3vwItWBO2o6t2GoDWuhv5UzX3yHQ'\n",
        "client = genai.Client(api_key= API_KEY)\n",
        "MODEL = 'gemini-2.5-flash'"
      ],
      "metadata": {
        "id": "txmDKy3CPbAN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Understanding the functions to call LLMs*"
      ],
      "metadata": {
        "id": "KSxqiKyvbL_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generate_with_single_input***"
      ],
      "metadata": {
        "id": "hSbWi8grQzZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_single_input(prompt, **kwargs):\n",
        "    # Gemini only accepts 'user' or 'model'.\n",
        "    # For a new request, the role MUST be 'user'.\n",
        "    role_input = kwargs.get('role', 'user')\n",
        "\n",
        "    # Safety check: If the user passes 'assistant', we treat it as 'user'\n",
        "    # to avoid the Gemini API 400 error for single-turn requests.\n",
        "    gemini_role = \"user\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=kwargs.get('model', \"gemini-2.5-flash\"),\n",
        "        contents=[{\"role\": gemini_role, \"parts\": [{\"text\": prompt}]}],\n",
        "        config=types.GenerateContentConfig(max_output_tokens=2000)\n",
        "    )\n",
        "\n",
        "    return {\"role\": role_input, \"content\": response.text}\n",
        "\n",
        "output = generate_with_single_input(\"Who is Genius, Billionaire, Philanthropist, Playboy\")\n",
        "print(f\"Role: {output['role']} \\n Content: {output['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs-q3QN9P6mU",
        "outputId": "edc0acfc-5018-4c19-9baa-23b3f0beeeb1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user \n",
            " Content: That description perfectly fits **Tony Stark**, also known as **Iron Man**, from Marvel Comics and the Marvel Cinematic Universe.\n",
            "\n",
            "He is famously known for being:\n",
            "*   **Genius:** A brilliant inventor, engineer, and futurist.\n",
            "*   **Billionaire:** Head of Stark Industries, a massive technology conglomerate.\n",
            "*   **Philanthropist:** Through the Stark Foundation and various initiatives, though often with his characteristic showmanship.\n",
            "*   **Playboy:** Especially in his earlier characterizations, known for his lavish lifestyle and numerous romantic interests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Generate_with_multiple_input***"
      ],
      "metadata": {
        "id": "CZr03-KIUo4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_multiple_input(messages, model=MODEL):\n",
        "    # Gemini uses 'user' and 'model' roles instead of 'user' and 'assistant'\n",
        "    formatted_messages = []\n",
        "    for m in messages:\n",
        "        role = \"model\" if m['role'] == \"assistant\" else m['role']\n",
        "        formatted_messages.append({\"role\": role, \"parts\": [{\"text\": m['content']}]})\n",
        "\n",
        "    # We take the last message as the new prompt and others as history\n",
        "    history = formatted_messages[:-1]\n",
        "    user_query = formatted_messages[-1]['parts'][0]['text']\n",
        "\n",
        "    chat = client.chats.create(model=model, history=history)\n",
        "    response = chat.send_message(user_query)\n",
        "\n",
        "    return {\"role\": \"model\", \"content\": response.text}\n",
        "\n",
        "messages = [\n",
        "    {'role': 'user', 'content': 'Hello, who won the FIFA world cup in 2018?'},\n",
        "    {'role': 'assistant', 'content': 'France won the 2018 FIFA World Cup.'},\n",
        "    {'role': 'user', 'content': 'who was the captain?'}\n",
        "  ]\n",
        "\n",
        "output = generate_with_multiple_input(messages)\n",
        "print(f\"Content: {output['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CY4FGAJTJjt",
        "outputId": "a5bdda67-323e-4ed9-e7d6-4efceab5013a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: The captain of the French team that won the 2018 FIFA World Cup was **Hugo Lloris**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating Data into an LLM Prompt\n",
        "\n",
        "*In this section, you will learn how to effectively incorporate data into a prompt before passing it to a Large Language Model (LLM). We will work with a small dataset consisting of JSON files that contain information about houses. It will help you understand how to augment prompts in the context of RAG*."
      ],
      "metadata": {
        "id": "mR7udDdSbfFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_data = [\n",
        "    {\n",
        "        \"address\": \"123 Main NY Street\",\n",
        "        \"city\": \"New York\",\n",
        "        \"state\": \"NY\",\n",
        "        \"zip\": \"10001\",\n",
        "        \"bedrooms\": 3,\n",
        "        \"bathrooms\": 2,\n",
        "        \"square_feet\": 1500,\n",
        "        \"year_built\": 1994,\n",
        "        \"price\": 230000\n",
        "    },\n",
        "    {\n",
        "        \"address\": \"456 Elm Avenue\",\n",
        "        \"city\": \"City of California\",\n",
        "        \"state\": \"California\",\n",
        "        \"zip\": \"10001\",\n",
        "        \"bedrooms\": 3,\n",
        "        \"bathrooms\": 2,\n",
        "        \"square_feet\": 1500,\n",
        "        \"year_built\": 1994,\n",
        "        \"price\": 320000\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "tQ9I0XaYatHE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Creating the Prompt*"
      ],
      "metadata": {
        "id": "-WIhP87rdVBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def house_info_layout(houses):\n",
        "    layout = ''\n",
        "    for house in houses:\n",
        "        # Corrected the price access and improved formatting\n",
        "        layout += (f\"- House at {house['address']}, {house['city']}, {house['state']} {house['zip']}: \"\n",
        "                   f\"{house['bedrooms']} bed, {house['bathrooms']} bath, {house['square_feet']} sqft, \"\n",
        "                   f\"built in {house['year_built']}, Price: ${house['price']}\\n\")\n",
        "    return layout\n",
        "\n",
        "# Check the layout\n",
        "print(house_info_layout(house_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2RkwWfUc_pi",
        "outputId": "7deb3201-754e-465d-fb29-28039cdd744a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- House at 123 Main NY Street, New York, NY 10001: 3 bed, 2 bath, 1500 sqft, built in 1994, Price: $230000\n",
            "- House at 456 Elm Avenue, City of California, California 10001: 3 bed, 2 bath, 1500 sqft, built in 1994, Price: $320000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Now create a function that generates the prompt to be passed to the Language Learning Model (LLM). The function will take a user-provided query and the available housing data as inputs to effectively address the user's query.*"
      ],
      "metadata": {
        "id": "Naip_mz7g9i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(query, houses):\n",
        "\n",
        "  houses_layout = house_info_layout(houses)\n",
        "  PROMPT = f\"\"\"\n",
        "  Use the following houses information to answer users queries.\n",
        "{houses_layout}\n",
        "Query: {query}\n",
        "  \"\"\"\n",
        "  return PROMPT"
      ],
      "metadata": {
        "id": "OBuvFhk8fPc7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the most expensive house? And the bigger one?\""
      ],
      "metadata": {
        "id": "88BVwqzAkxNn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking without the augmented prompt, let's pass the role as user\n",
        "query_without_house_info = generate_with_single_input(prompt = query, role = 'user')"
      ],
      "metadata": {
        "id": "HjyrDPQdkvq8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With house info, given the prompt structuer, let's pass the role as assistant\n",
        "enhanced_query = generate_prompt(query, houses = house_data)\n",
        "query_with_house_info = generate_with_single_input(prompt = enhanced_query, role = 'assistant')\n",
        "\n",
        "print(query_with_house_info['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoCoTAFYhzGu",
        "outputId": "2ec1371d-410d-40bf-9c65-fe5f4305ba5f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided:\n",
            "\n",
            "*   The most expensive house is the one at **456 Elm Avenue, City of California, California 10001**, priced at **$320,000**.\n",
            "*   Both houses are the **same size (1500 sqft)**, so there isn't a \"bigger one\" between them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mtuh3dkqiYI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}